name: Data Pipeline

on:
  workflow_dispatch:
    inputs:
      region:
        description: "AWS Region"
        required: true
        default: "ap-southeast-2"
      SourceBucketName:
        description: "S3 Source Bucket Name"
        default: "source-bucket-chien"
        required: true
      DataBucketName:
        description: "S3 Data Bucket Name"
        default: "data-bucket-chien"
      csvCrawler:
        description: "Run csv crawler Job?"
        required: true
        default: "false"

jobs:
  pipeline:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Configure AWS credentials
        id: creds-pipeline
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ github.event.inputs.region }}

      - name: Run crawler script - input csv
        if: ${{ github.event.inputs.csvCrawler == 'true' }}
        run: |
          bash pipeline/run_crawler.sh
        env:
          CrawlerName: source-csv-crawler

      # - name: Run Glue job and wait
      #   run: ./scripts/glue_runner.sh sales-aggregation-job

      - name: Run Glue job and wait - snowflake exports
        run: ./pipeline/glue_runner.sh snowflake-ingest

      - name: Run crawler script - snowflake exports
        run: |
          bash pipeline/run_crawler.sh
        env:
          CrawlerName: snowflake_input_crawler
